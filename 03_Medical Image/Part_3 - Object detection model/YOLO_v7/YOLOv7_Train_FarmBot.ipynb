{"cells":[{"cell_type":"markdown","source":["<a href=\"https://colab.research.google.com/github/stuser/Python_md/blob/master/object_detection/YOLO_v7/YOLOv7_Train_FarmBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"],"metadata":{"id":"5FolrgfORAnO"},"id":"5FolrgfORAnO"},{"cell_type":"markdown","source":["## AIA&FBTUG – 資料標記與模型訓練(yolo)\n","- AIA&FBTUG專案說明簡報檔案連結:\n","https://drive.google.com/open?id=1WBD60MDXIpr1XpBloE1fy2RQ0k72qrr9inT-ggNkCMc\n","\n","- 學員成果簡報影片連結:\n","https://youtu.be/iY2RZGmV3sY\n","\n","- Yolo模型成果影片連結:\n","https://youtu.be/Bzf8ZjIEqGI\n","\n","- Yolo模型在realtime coco資料集的排名\n","https://paperswithcode.com/sota/real-time-object-detection-on-coco"],"metadata":{"id":"wr2WxUVEwdx0"},"id":"wr2WxUVEwdx0"},{"cell_type":"markdown","source":["# YOLOv7 模型架構\n","\n","<img src=\"https://github.com/stuser/Python_md/blob/master/object_detection/YOLO_v7/pic/yolov7_model.png?raw=true\"  width=\"640\" height=\"320\">\n"],"metadata":{"id":"Z5DZstI_z9bp"},"id":"Z5DZstI_z9bp"},{"cell_type":"markdown","source":["### 執行環境"],"metadata":{"id":"PUvxUyylExrz"},"id":"PUvxUyylExrz"},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PfUlYAdhwlqz","executionInfo":{"status":"ok","timestamp":1685857478650,"user_tz":-480,"elapsed":347,"user":{"displayName":"林德全","userId":"14520221228179753968"}},"outputId":"99eff257-b2b6-42f4-bee1-30251151978d"},"id":"PfUlYAdhwlqz","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Jun  4 05:44:37 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   55C    P8    12W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["# verify CUDA\n","!/usr/local/cuda/bin/nvcc --version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w4ubtNRTugPW","executionInfo":{"status":"ok","timestamp":1685857482102,"user_tz":-480,"elapsed":389,"user":{"displayName":"林德全","userId":"14520221228179753968"}},"outputId":"a582974c-90fe-4063-b549-056a336dd197"},"id":"w4ubtNRTugPW","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2022 NVIDIA Corporation\n","Built on Wed_Sep_21_10:33:58_PDT_2022\n","Cuda compilation tools, release 11.8, V11.8.89\n","Build cuda_11.8.r11.8/compiler.31833905_0\n"]}]},{"cell_type":"markdown","id":"419e9476","metadata":{"id":"419e9476"},"source":["### 下載課程所需檔案 (YOLOv7, Dataset)"]},{"cell_type":"markdown","source":["#### YOLOv7(Github程式檔)"],"metadata":{"id":"LAspwPUaFSuZ"},"id":"LAspwPUaFSuZ"},{"cell_type":"code","source":["!git clone https://github.com/WongKinYiu/yolov7.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AbV05N-f785a","executionInfo":{"status":"ok","timestamp":1685857491723,"user_tz":-480,"elapsed":2835,"user":{"displayName":"林德全","userId":"14520221228179753968"}},"outputId":"1949b8e7-11bb-42c6-d586-9abfb00debe8"},"id":"AbV05N-f785a","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'yolov7'...\n","remote: Enumerating objects: 1185, done.\u001b[K\n","remote: Total 1185 (delta 0), reused 0 (delta 0), pack-reused 1185\u001b[K\n","Receiving objects: 100% (1185/1185), 74.23 MiB | 34.87 MiB/s, done.\n","Resolving deltas: 100% (512/512), done.\n"]}]},{"cell_type":"code","source":["%pip install -qr /content/yolov7/requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ciLlxQZk8ES7","executionInfo":{"status":"ok","timestamp":1685857504054,"user_tz":-480,"elapsed":5176,"user":{"displayName":"林德全","userId":"14520221228179753968"}},"outputId":"cf971521-41fa-4251-b620-c96231768489"},"id":"ciLlxQZk8ES7","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/1.6 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["%matplotlib inline\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import os\n","import glob\n","import random\n","import PIL\n","import sys\n","from IPython.display import Image\n","\n","#把 yolov7 這個資料夾設成 Python 是找得到的路徑\n","sys.path.insert(0,'./yolov7')"],"metadata":{"id":"rTW7mI8L8VMx","executionInfo":{"status":"ok","timestamp":1685857510613,"user_tz":-480,"elapsed":779,"user":{"displayName":"林德全","userId":"14520221228179753968"}}},"id":"rTW7mI8L8VMx","execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["#### FarmBot-VD03(彩椒資料集)"],"metadata":{"id":"s3mSfd_cFVUR"},"id":"s3mSfd_cFVUR"},{"cell_type":"markdown","source":["FarmBot-彩椒照片資料集(665mb)連結: https://drive.google.com/file/d/1zrR3-6YBXCWVDp-GDx9D0vcMeIA2vM74/view?usp=share_link\n"],"metadata":{"id":"aj7x1d1IuOoN"},"id":"aj7x1d1IuOoN"},{"cell_type":"code","source":["!pip install -q gdown\n","\n","!gdown --id 1zrR3-6YBXCWVDp-GDx9D0vcMeIA2vM74"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Bjvm1KHuImG","executionInfo":{"status":"ok","timestamp":1685857526812,"user_tz":-480,"elapsed":12409,"user":{"displayName":"林德全","userId":"14520221228179753968"}},"outputId":"d6bb5640-286e-4241-a81b-b8b492e86747"},"id":"5Bjvm1KHuImG","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  warnings.warn(\n","Downloading...\n","From: https://drive.google.com/uc?id=1zrR3-6YBXCWVDp-GDx9D0vcMeIA2vM74\n","To: /content/VD03.zip\n","100% 665M/665M [00:07<00:00, 94.5MB/s]\n"]}]},{"cell_type":"code","source":["!unzip VD03.zip > logs"],"metadata":{"id":"XcF8TdnlwQyM","executionInfo":{"status":"ok","timestamp":1685857538404,"user_tz":-480,"elapsed":7834,"user":{"displayName":"林德全","userId":"14520221228179753968"}}},"id":"XcF8TdnlwQyM","execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["手動整理一下資料匣:\n","把VD03資料匣移入yolov7資料匣內,方便模型訓練叫用。\n","\n","標記的類別:\n","- 0_pepper_flower\n","- 1_pepper_young\n","- 2_pepper_matured\n","- 3_pepper_covered"],"metadata":{"id":"JxQUiaEk142H"},"id":"JxQUiaEk142H"},{"cell_type":"code","source":["peper_label_list = ['0_pepper_flower','1_pepper_young','2_pepper_matured','3_pepper_covered']"],"metadata":{"id":"61T_MjCexWB-","executionInfo":{"status":"ok","timestamp":1685857542484,"user_tz":-480,"elapsed":366,"user":{"displayName":"林德全","userId":"14520221228179753968"}}},"id":"61T_MjCexWB-","execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"P1yUcE9KLYOk"},"id":"P1yUcE9KLYOk"},{"cell_type":"markdown","id":"0b363a9b","metadata":{"id":"0b363a9b"},"source":["# YOLOv7 實作\n"]},{"cell_type":"markdown","id":"e68b2961","metadata":{"id":"e68b2961"},"source":["## 1. 確認資料集格式\n","   \n","![](https://albumentations.ai/docs/images/getting_started/augmenting_bboxes/bbox_formats.jpg)\n","\n","依照上圖yolo的BBox的座標表示計算為:\n","\n","[((420 + 98) / 2) / 640, ((462 + 345) / 2) / 480, 322 / 640, 117 / 480] (需依照片尺寸640*480正規化) \n","\n","得到: [0.4046875, 0.840625, 0.503125, 0.24375]\n","\n","(source: [albumentations.ai](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/#yolo))"]},{"cell_type":"code","execution_count":9,"id":"acebedea","metadata":{"id":"acebedea","executionInfo":{"status":"ok","timestamp":1685857548355,"user_tz":-480,"elapsed":348,"user":{"displayName":"林德全","userId":"14520221228179753968"}}},"outputs":[],"source":["\n","name = 'VD03'  # 資料集名稱\n","classes = peper_label_list  # 修改自己的類別\n","\n","train_image_path = f'{name}/train/images/'\n","train_label_path = f'{name}/train/labels/'\n","valid_image_path = f'{name}/valid/images/'\n","valid_label_path = f'{name}/valid/labels/'\n","\n","if not os.path.exists(train_image_path):\n","    os.makedirs(train_image_path)\n","if not os.path.exists(train_label_path):\n","    os.makedirs(train_label_path)\n","if not os.path.exists(valid_image_path):\n","    os.makedirs(valid_image_path)\n","if not os.path.exists(valid_label_path):\n","    os.makedirs(valid_label_path)"]},{"cell_type":"markdown","id":"8cd2d1f3","metadata":{"id":"8cd2d1f3"},"source":["## 2. 更改設定檔案(yaml)\n","yolov7有三個基本的設定檔要設定:\n","- (1) 依照 cfg/training/yolov7.yaml 製作模型訓練設定 yaml檔 (有P5/P6/tiny的版本)\n","- (2) 依照 data/coco.yaml 製作一個資料集設定 yaml檔\n","- (3) 依照 data/hyp.scratch.p5.yaml 模型超參數設定 yaml檔 (有P5/P6/tiny的版本)"]},{"cell_type":"markdown","id":"3beea9c9","metadata":{"id":"3beea9c9"},"source":["將yolov7.yaml 設定檔複製一份\n"," \n","!cp 要複製的檔案 新檔案名稱"]},{"cell_type":"code","execution_count":10,"id":"1da4d0ea","metadata":{"id":"1da4d0ea","executionInfo":{"status":"ok","timestamp":1685857554490,"user_tz":-480,"elapsed":400,"user":{"displayName":"林德全","userId":"14520221228179753968"}}},"outputs":[],"source":["!cp yolov7/cfg/training/yolov7.yaml yolov7/cfg/training/yolov7-VD03.yaml"]},{"cell_type":"markdown","id":"eecaf7e9","metadata":{"id":"eecaf7e9"},"source":["將class的地方改成自己的class數量\n","- 你可以手動去修改yaml文件檔案\n","- 或是使用以下指令(sed)來修改yaml文件檔\n","\n","!sed -n -e (顯示) 第幾行 檔案名稱"]},{"cell_type":"code","execution_count":11,"id":"4d0c055f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4d0c055f","executionInfo":{"status":"ok","timestamp":1685857558799,"user_tz":-480,"elapsed":391,"user":{"displayName":"林德全","userId":"14520221228179753968"}},"outputId":"6fbabcf5-e6cc-4c41-c81d-a271832b365e"},"outputs":[{"output_type":"stream","name":"stdout","text":["nc: 80  # number of classes\n"]}],"source":["!sed -n -e 2p yolov7/cfg/training/yolov7-VD03.yaml"]},{"cell_type":"markdown","id":"22a4fc1a","metadata":{"id":"22a4fc1a"},"source":["#### (1) 依照 cfg/training/yolov7.yaml 製作\n","\n","!sed -i (修改) 第幾行/欲修改的字/目標字/ 檔案名稱"]},{"cell_type":"code","execution_count":12,"id":"034cd458","metadata":{"id":"034cd458","executionInfo":{"status":"ok","timestamp":1685857560749,"user_tz":-480,"elapsed":378,"user":{"displayName":"林德全","userId":"14520221228179753968"}}},"outputs":[],"source":["!sed -i '2s/80/4/' yolov7/cfg/training/yolov7-VD03.yaml"]},{"cell_type":"code","execution_count":13,"id":"dadf99a6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dadf99a6","executionInfo":{"status":"ok","timestamp":1685857563606,"user_tz":-480,"elapsed":446,"user":{"displayName":"林德全","userId":"14520221228179753968"}},"outputId":"114064a3-b49a-4efd-d638-8d3b75e33987"},"outputs":[{"output_type":"stream","name":"stdout","text":["nc: 4  # number of classes\n"]}],"source":["!sed -n -e 2p yolov7/cfg/training/yolov7-VD03.yaml"]},{"cell_type":"markdown","id":"2e1a0f59","metadata":{"id":"2e1a0f59"},"source":["#### (2) 依照 data/coco.yaml 製作\n","參考data/coco.yaml 製作一個自己資料集的yaml"]},{"cell_type":"code","execution_count":14,"id":"61c7bdde","metadata":{"id":"61c7bdde","executionInfo":{"status":"ok","timestamp":1685857566762,"user_tz":-480,"elapsed":403,"user":{"displayName":"林德全","userId":"14520221228179753968"}}},"outputs":[],"source":["text = \\\n","    \"\"\"\n","    train: ./VD03/train # 訓練資料夾位置\n","    val: ./VD03/valid # 驗證資料夾位置\n","    test: ./VD03/valid # 測試資料夾位置\n","\n","    # number of classes\n","    nc: 4 # <-需修改成自己的類別數量\n","\n","    # class names\n","    names: ['0_pepper_flower','1_pepper_young','2_pepper_matured','3_pepper_covered']\n","    \"\"\""]},{"cell_type":"code","execution_count":15,"id":"532bbd90","metadata":{"id":"532bbd90","executionInfo":{"status":"ok","timestamp":1685857570653,"user_tz":-480,"elapsed":258,"user":{"displayName":"林德全","userId":"14520221228179753968"}}},"outputs":[],"source":["with open(f'yolov7/data/{name}.yaml', 'w') as file:\n","    file.write(text)"]},{"cell_type":"markdown","source":["#### (3) 依照 data/hyp.scratch.p5.yaml 製作\n","客製化訓練的超參數設定"],"metadata":{"id":"hjx2YVTVH6aZ"},"id":"hjx2YVTVH6aZ"},{"cell_type":"code","source":["!cp yolov7/data/hyp.scratch.custom.yaml yolov7/data/hyp.VD03.yaml"],"metadata":{"id":"bbJf36oaH6_I","executionInfo":{"status":"ok","timestamp":1685857573841,"user_tz":-480,"elapsed":347,"user":{"displayName":"林德全","userId":"14520221228179753968"}}},"id":"bbJf36oaH6_I","execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"SqFTfRSbItuP"},"id":"SqFTfRSbItuP"},{"cell_type":"markdown","id":"d8782513","metadata":{"id":"d8782513"},"source":["## 模型訓練"]},{"cell_type":"markdown","id":"8f97dea5","metadata":{"id":"8f97dea5"},"source":["#### 下載預訓練權重檔案\n","\n","下載預訓練權重檔案\n","https://github.com/WongKinYiu/yolov7\n","\n","release v1.0 ([https://github.com/WongKinYiu/yolov7/releases/tag/v0.1](https://github.com/WongKinYiu/yolov7/releases/tag/v0.1))\n","\n","**Transfer learning**\n","\n","可供下載的預訓練權重檔名如下:\n","- yolov7.pt (註:預設是P5的模型架構)\n","- yolov7_training.pt (P5)\n","- yolov7x_training.pt (P5)\n","- yolov7-w6_training.pt (註:這是P6模型架構的版本)\n","- yolov7-e6_training.pt (P6)\n","- yolov7-d6_training.pt (P6)\n","- yolov7-e6e_training.pt (P6)\n","\n","請將權重檔案放置於yolov7/weights/資料夾底下"]},{"cell_type":"code","source":["!wget -P ./yolov7/weights https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7_training.pt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G8_NyYdp4xHe","executionInfo":{"status":"ok","timestamp":1685857578929,"user_tz":-480,"elapsed":762,"user":{"displayName":"林德全","userId":"14520221228179753968"}},"outputId":"82b58864-4d5a-4b52-8894-30fc6d15105d"},"id":"G8_NyYdp4xHe","execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-06-04 05:46:17--  https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7_training.pt\n","Resolving github.com (github.com)... 140.82.114.4\n","Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/511187726/13e046d1-f7f0-43ab-910b-480613181b1f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230604%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230604T054617Z&X-Amz-Expires=300&X-Amz-Signature=96893f6c84152a73c50ec75b0b285257ed4fb0d2900fb774a1f1f2ffb8b9c28d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=511187726&response-content-disposition=attachment%3B%20filename%3Dyolov7_training.pt&response-content-type=application%2Foctet-stream [following]\n","--2023-06-04 05:46:17--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/511187726/13e046d1-f7f0-43ab-910b-480613181b1f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230604%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230604T054617Z&X-Amz-Expires=300&X-Amz-Signature=96893f6c84152a73c50ec75b0b285257ed4fb0d2900fb774a1f1f2ffb8b9c28d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=511187726&response-content-disposition=attachment%3B%20filename%3Dyolov7_training.pt&response-content-type=application%2Foctet-stream\n","Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n","Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 75628875 (72M) [application/octet-stream]\n","Saving to: ‘./yolov7/weights/yolov7_training.pt’\n","\n","yolov7_training.pt  100%[===================>]  72.12M   209MB/s    in 0.3s    \n","\n","2023-06-04 05:46:18 (209 MB/s) - ‘./yolov7/weights/yolov7_training.pt’ saved [75628875/75628875]\n","\n"]}]},{"cell_type":"markdown","id":"4854c4d3","metadata":{"id":"4854c4d3"},"source":["#### 模型訓練參數\n","執行訓練，訓練參數介紹：\n","- --weights : 預先訓練的權重路徑(weights/yolov7_training.pt)\n","- --cfg：模型設定檔案路徑(cfg/training/yolov7-VD03.yaml)\n","- --data：資料集設定檔案路徑(data/VD03.yaml)\n","- --device：GPU設定(單張GPU時,設為0)\n","- --batch-size：一次訓練照片張數\n","- --epoch： 訓練回合數\n","\n","其他可調控參數可置train.py中察看\n","\n","### Single GPU finetuning for custom dataset\n","\n","```\n","# finetune p5 models\n","python train.py --workers 8 --device 0 --batch-size 32 --data data/custom.yaml --img 640 640 --cfg cfg/training/yolov7-custom.yaml --weights 'yolov7_training.pt' --name yolov7-custom --hyp data/hyp.scratch.custom.yaml\n","\n","# finetune p6 models\n","python train_aux.py --workers 8 --device 0 --batch-size 16 --data data/custom.yaml --img 1280 1280 --cfg cfg/training/yolov7-w6-custom.yaml --weights 'yolov7-w6_training.pt' --name yolov7-w6-custom --hyp data/hyp.scratch.custom.yaml\n","```\n","\n"]},{"cell_type":"markdown","source":["**(重要)因為要在yolov7資料匣內執行train.py程式,故需確認以下事項：**\n","- 在Colab使用環境下，**使用%cd更改當前目錄位置到 yolov7資料匣**.\n","- **把VD03資料匣拉到yolov7資料匣裡面**."],"metadata":{"id":"M8e3Q5xWAok7"},"id":"M8e3Q5xWAok7"},{"cell_type":"code","source":["%cd yolov7"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"plGTtKoEAi4F","executionInfo":{"status":"ok","timestamp":1685857604747,"user_tz":-480,"elapsed":252,"user":{"displayName":"林德全","userId":"14520221228179753968"}},"outputId":"6f1e3d82-d6db-4254-a6eb-e4a78e2dcde5"},"id":"plGTtKoEAi4F","execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/yolov7\n"]}]},{"cell_type":"markdown","source":["(**重要)如果你要使用P6模型架構做訓練，請修改(utils/loss.py)以下兩行程式碼**\n","\n","If you're training P6 models like e6 or w6 or x, then you'll need to change the following lines as well:\n","\n","(reference: https://github.com/WongKinYiu/yolov7/issues/1101)\n","\n","```\n","1389 - matching_matrix = torch.zeros_like(cost) to matching_matrix = torch.zeros_like(cost, device=\"cpu\")\n","\n","1543 - matching_matrix = torch.zeros_like(cost) to matching_matrix = torch.zeros_like(cost, device=\"cpu\")\n","\n","```\n","in the same file (utils/loss.py)."],"metadata":{"id":"L9OESojFIKNC"},"id":"L9OESojFIKNC"},{"cell_type":"markdown","source":["#### 進行模型訓練"],"metadata":{"id":"aGQDaTRhJcN1"},"id":"aGQDaTRhJcN1"},{"cell_type":"code","execution_count":19,"id":"bcb164a1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bcb164a1","executionInfo":{"status":"ok","timestamp":1685859238342,"user_tz":-480,"elapsed":1629555,"user":{"displayName":"林德全","userId":"14520221228179753968"}},"outputId":"915e1f04-c60d-46aa-a2a7-8239c85ea3f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-04 05:46:52.374770: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-06-04 05:46:53.236450: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","YOLOR 🚀 v0.1-126-g84932d7 torch 2.0.1+cu118 CUDA:0 (Tesla T4, 15101.8125MB)\n","\n","Namespace(weights='weights/yolov7_training.pt', cfg='cfg/training/yolov7-VD03.yaml', data='data/VD03.yaml', hyp='data/hyp.scratch.p5.yaml', epochs=10, batch_size=8, img_size=[1024, 1024], rect=False, resume=False, nosave=False, notest=False, noautoanchor=False, evolve=False, bucket='', cache_images=False, image_weights=False, device='0', multi_scale=False, single_cls=False, adam=False, sync_bn=False, local_rank=-1, workers=8, project='runs/train', entity=None, name='exp', exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias='latest', freeze=[0], v5_metric=False, world_size=1, global_rank=-1, save_dir='runs/train/exp', total_batch_size=8)\n","\u001b[34m\u001b[1mtensorboard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.3, cls_pw=1.0, obj=0.7, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.2, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.0, paste_in=0.15, loss_ota=1\n","\u001b[34m\u001b[1mwandb: \u001b[0mInstall Weights & Biases for YOLOR logging with 'pip install wandb' (recommended)\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1       928  models.common.Conv                      [3, 32, 3, 1]                 \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n","  5                -2  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n","  6                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n","  7                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n","  8                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n","  9                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n"," 10  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 11                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n"," 12                -1  1         0  models.common.MP                        []                            \n"," 13                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 14                -3  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 16          [-1, -3]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 18                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 19                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 20                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 22                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 23  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 24                -1  1    263168  models.common.Conv                      [512, 512, 1, 1]              \n"," 25                -1  1         0  models.common.MP                        []                            \n"," 26                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 27                -3  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 28                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 29          [-1, -3]  1         0  models.common.Concat                    [1]                           \n"," 30                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 31                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 32                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 33                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 34                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 35                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 36  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 37                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n"," 38                -1  1         0  models.common.MP                        []                            \n"," 39                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n"," 40                -3  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n"," 41                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n"," 42          [-1, -3]  1         0  models.common.Concat                    [1]                           \n"," 43                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 44                -2  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 45                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 46                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 47                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 48                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 49  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 50                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n"," 51                -1  1   7609344  models.common.SPPCSPC                   [1024, 512, 1]                \n"," 52                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 53                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 54                37  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 55          [-1, -2]  1         0  models.common.Concat                    [1]                           \n"," 56                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 57                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 58                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n"," 59                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 60                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 61                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 62[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 63                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 64                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 65                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 66                24  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n"," 67          [-1, -2]  1         0  models.common.Concat                    [1]                           \n"," 68                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 69                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 70                -1  1     73856  models.common.Conv                      [128, 64, 3, 1]               \n"," 71                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n"," 72                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n"," 73                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n"," 74[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 75                -1  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n"," 76                -1  1         0  models.common.MP                        []                            \n"," 77                -1  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n"," 78                -3  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n"," 79                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 80      [-1, -3, 63]  1         0  models.common.Concat                    [1]                           \n"," 81                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 82                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 83                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n"," 84                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 85                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 86                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 87[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 88                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 89                -1  1         0  models.common.MP                        []                            \n"," 90                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n"," 91                -3  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n"," 92                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 93      [-1, -3, 51]  1         0  models.common.Concat                    [1]                           \n"," 94                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n"," 95                -2  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n"," 96                -1  1   1180160  models.common.Conv                      [512, 256, 3, 1]              \n"," 97                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 98                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 99                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n","100[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n","101                -1  1   1049600  models.common.Conv                      [2048, 512, 1, 1]             \n","102                75  1    328704  models.common.RepConv                   [128, 256, 3, 1]              \n","103                88  1   1312768  models.common.RepConv                   [256, 512, 3, 1]              \n","104               101  1   5246976  models.common.RepConv                   [512, 1024, 3, 1]             \n","105   [102, 103, 104]  1     50338  models.yolo.IDetect                     [4, [[12, 16, 19, 36, 40, 28], [36, 75, 76, 55, 72, 146], [142, 110, 192, 243, 459, 401]], [256, 512, 1024]]\n","/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","Model Summary: 415 layers, 37212738 parameters, 37212738 gradients, 105.2 GFLOPS\n","\n","Transferred 555/566 items from weights/yolov7_training.pt\n","Scaled weight_decay = 0.0005\n","Optimizer groups: 95 .bias, 95 conv.weight, 98 other\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'VD03/train/labels' images and labels... 457 found, 2 missing, 0 empty, 0 corrupted: 100% 459/459 [00:00<00:00, 978.98it/s] \n","\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: VD03/train/labels.cache\n","\u001b[34m\u001b[1mval: \u001b[0mScanning 'VD03/valid/labels' images and labels... 164 found, 2 missing, 0 empty, 0 corrupted: 100% 166/166 [00:00<00:00, 566.67it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mNew cache created: VD03/valid/labels.cache\n","\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 5.43, Best Possible Recall (BPR) = 1.0000\n","Image sizes 1024 train, 1024 test\n","Using 2 dataloader workers\n","Logging results to runs/train/exp\n","Starting training for 10 epochs...\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       0/9     2.22G   0.07181   0.03065   0.02518    0.1276         3      1024: 100% 58/58 [02:48<00:00,  2.91s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:17<00:00,  1.57s/it]\n","                 all         166         606     0.00661      0.0675     0.00264    0.000451\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       1/9     13.6G   0.06328   0.02234   0.02216    0.1078        24      1024: 100% 58/58 [02:12<00:00,  2.28s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:13<00:00,  1.27s/it]\n","                 all         166         606      0.0441       0.182      0.0322     0.00757\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       2/9     13.6G   0.05396   0.02101   0.01969   0.09467        12      1024: 100% 58/58 [02:15<00:00,  2.34s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:13<00:00,  1.19s/it]\n","                 all         166         606       0.445       0.204       0.137       0.045\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       3/9     13.6G   0.04937   0.01929   0.01745   0.08611        21      1024: 100% 58/58 [02:15<00:00,  2.33s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:13<00:00,  1.25s/it]\n","                 all         166         606       0.121       0.331       0.111      0.0371\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       4/9     13.7G   0.05119   0.01747   0.01729   0.08596        37      1024: 100% 58/58 [02:13<00:00,  2.31s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:14<00:00,  1.36s/it]\n","                 all         166         606       0.444       0.381        0.21       0.112\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       5/9     13.7G   0.04296   0.01622   0.01657   0.07575        14      1024: 100% 58/58 [02:10<00:00,  2.26s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:13<00:00,  1.25s/it]\n","                 all         166         606       0.192       0.372       0.239       0.106\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       6/9     13.7G   0.04267   0.01536   0.01523   0.07326         4      1024: 100% 58/58 [02:09<00:00,  2.23s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:13<00:00,  1.19s/it]\n","                 all         166         606       0.311       0.411       0.295       0.117\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       7/9     13.7G   0.03924   0.01466   0.01503   0.06893        19      1024: 100% 58/58 [02:10<00:00,  2.25s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:14<00:00,  1.33s/it]\n","                 all         166         606       0.296        0.47        0.32       0.186\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       8/9     13.7G    0.0381   0.01551   0.01357   0.06718        12      1024: 100% 58/58 [02:17<00:00,  2.38s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:13<00:00,  1.25s/it]\n","                 all         166         606       0.303       0.393       0.293       0.157\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       9/9     13.7G   0.03665   0.01501   0.01315   0.06481        19      1024: 100% 58/58 [02:13<00:00,  2.30s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:17<00:00,  1.58s/it]\n","                 all         166         606        0.35       0.539       0.369       0.217\n","     0_pepper_flower         166           8      0.0988        0.25      0.0364      0.0114\n","      1_pepper_young         166         430       0.507       0.751       0.661       0.396\n","    2_pepper_matured         166          35        0.26         0.6       0.317       0.228\n","    3_pepper_covered         166         133       0.534       0.556       0.461       0.234\n","10 epochs completed in 0.446 hours.\n","\n","Optimizer stripped from runs/train/exp/weights/last.pt, 74.9MB\n","Optimizer stripped from runs/train/exp/weights/best.pt, 74.9MB\n"]}],"source":["# finetune p5 models\n","!python train.py --weights weights/yolov7_training.pt --img 1024 1024 --cfg cfg/training/yolov7-VD03.yaml --data data/VD03.yaml --device 0 --batch-size 8 --epoch 10\n"]},{"cell_type":"markdown","source":["以下是給retrain方便呼叫使用"],"metadata":{"id":"hEzBH_0IJ_2w"},"id":"hEzBH_0IJ_2w"},{"cell_type":"code","source":["#!python train.py --weights runs/train/exp/weights/best.pt --img 1024 1024 --cfg cfg/training/yolov7-VD03.yaml --data data/VD03.yaml --device 0 --batch-size 8 --epoch 10"],"metadata":{"id":"RYhvnBNaNZZd","executionInfo":{"status":"ok","timestamp":1683275730974,"user_tz":-480,"elapsed":795857,"user":{"displayName":"tc Lin","userId":"05338448855796845949"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"aac33359-1371-4e7d-d80e-c84743fa2a37"},"id":"RYhvnBNaNZZd","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-05-05 08:22:17.352712: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-05-05 08:22:18.382128: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","YOLOR 🚀 v0.1-122-g3b41c2c torch 2.0.0+cu118 CUDA:0 (Tesla V100-SXM2-16GB, 16150.875MB)\n","\n","Namespace(weights='runs/train/exp/weights/best.pt', cfg='cfg/training/yolov7-VD03.yaml', data='data/VD03.yaml', hyp='data/hyp.scratch.p5.yaml', epochs=10, batch_size=8, img_size=[1024, 1024], rect=False, resume=False, nosave=False, notest=False, noautoanchor=False, evolve=False, bucket='', cache_images=False, image_weights=False, device='0', multi_scale=False, single_cls=False, adam=False, sync_bn=False, local_rank=-1, workers=8, project='runs/train', entity=None, name='exp', exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias='latest', freeze=[0], v5_metric=False, world_size=1, global_rank=-1, save_dir='runs/train/exp2', total_batch_size=8)\n","\u001b[34m\u001b[1mtensorboard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.3, cls_pw=1.0, obj=0.7, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.2, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.0, paste_in=0.15, loss_ota=1\n","\u001b[34m\u001b[1mwandb: \u001b[0mInstall Weights & Biases for YOLOR logging with 'pip install wandb' (recommended)\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1       928  models.common.Conv                      [3, 32, 3, 1]                 \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n","  5                -2  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n","  6                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n","  7                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n","  8                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n","  9                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n"," 10  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 11                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n"," 12                -1  1         0  models.common.MP                        []                            \n"," 13                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 14                -3  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 16          [-1, -3]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 18                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 19                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 20                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 22                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 23  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 24                -1  1    263168  models.common.Conv                      [512, 512, 1, 1]              \n"," 25                -1  1         0  models.common.MP                        []                            \n"," 26                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 27                -3  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 28                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 29          [-1, -3]  1         0  models.common.Concat                    [1]                           \n"," 30                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 31                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 32                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 33                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 34                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 35                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 36  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 37                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n"," 38                -1  1         0  models.common.MP                        []                            \n"," 39                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n"," 40                -3  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n"," 41                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n"," 42          [-1, -3]  1         0  models.common.Concat                    [1]                           \n"," 43                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 44                -2  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 45                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 46                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 47                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 48                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 49  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 50                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n"," 51                -1  1   7609344  models.common.SPPCSPC                   [1024, 512, 1]                \n"," 52                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 53                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 54                37  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 55          [-1, -2]  1         0  models.common.Concat                    [1]                           \n"," 56                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 57                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 58                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n"," 59                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 60                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 61                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 62[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 63                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 64                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 65                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 66                24  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n"," 67          [-1, -2]  1         0  models.common.Concat                    [1]                           \n"," 68                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 69                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 70                -1  1     73856  models.common.Conv                      [128, 64, 3, 1]               \n"," 71                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n"," 72                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n"," 73                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n"," 74[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 75                -1  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n"," 76                -1  1         0  models.common.MP                        []                            \n"," 77                -1  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n"," 78                -3  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n"," 79                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 80      [-1, -3, 63]  1         0  models.common.Concat                    [1]                           \n"," 81                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 82                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 83                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n"," 84                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 85                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 86                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 87[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 88                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 89                -1  1         0  models.common.MP                        []                            \n"," 90                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n"," 91                -3  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n"," 92                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 93      [-1, -3, 51]  1         0  models.common.Concat                    [1]                           \n"," 94                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n"," 95                -2  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n"," 96                -1  1   1180160  models.common.Conv                      [512, 256, 3, 1]              \n"," 97                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 98                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 99                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n","100[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n","101                -1  1   1049600  models.common.Conv                      [2048, 512, 1, 1]             \n","102                75  1    328704  models.common.RepConv                   [128, 256, 3, 1]              \n","103                88  1   1312768  models.common.RepConv                   [256, 512, 3, 1]              \n","104               101  1   5246976  models.common.RepConv                   [512, 1024, 3, 1]             \n","105   [102, 103, 104]  1     50338  models.yolo.IDetect                     [4, [[12, 16, 19, 36, 40, 28], [36, 75, 76, 55, 72, 146], [142, 110, 192, 243, 459, 401]], [256, 512, 1024]]\n","/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","Model Summary: 415 layers, 37212738 parameters, 37212738 gradients, 105.2 GFLOPS\n","\n","Transferred 564/566 items from runs/train/exp/weights/best.pt\n","Scaled weight_decay = 0.0005\n","Optimizer groups: 95 .bias, 95 conv.weight, 98 other\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'VD03/train/labels.cache' images and labels... 457 found, 2 missing, 0 empty, 0 corrupted: 100% 459/459 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning 'VD03/valid/labels.cache' images and labels... 164 found, 2 missing, 0 empty, 0 corrupted: 100% 166/166 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 5.43, Best Possible Recall (BPR) = 1.0000\n","Image sizes 1024 train, 1024 test\n","Using 4 dataloader workers\n","Logging results to runs/train/exp2\n","Starting training for 10 epochs...\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       0/9     3.16G   0.04084    0.0179   0.01696    0.0757        11      1024: 100% 58/58 [01:30<00:00,  1.57s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:06<00:00,  1.64it/s]\n","                 all         166         606       0.466       0.362       0.241       0.132\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       1/9     14.5G   0.04259   0.01771   0.01742   0.07771         7      1024: 100% 58/58 [01:05<00:00,  1.13s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:06<00:00,  1.78it/s]\n","                 all         166         606       0.432       0.384       0.183      0.0815\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       2/9     14.5G   0.04413   0.01633   0.01594   0.07641        18      1024: 100% 58/58 [01:05<00:00,  1.13s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:05<00:00,  1.97it/s]\n","                 all         166         606       0.464       0.307       0.204      0.0882\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       3/9     14.5G   0.04365   0.01429   0.01607   0.07401         7      1024: 100% 58/58 [01:01<00:00,  1.06s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:05<00:00,  2.09it/s]\n","                 all         166         606       0.151       0.333       0.142      0.0429\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       4/9     14.5G   0.04786   0.01524   0.01553   0.07862        19      1024: 100% 58/58 [01:10<00:00,  1.22s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:05<00:00,  2.06it/s]\n","                 all         166         606       0.258       0.438        0.31       0.169\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       5/9     14.5G    0.0421   0.01625   0.01497   0.07331        25      1024: 100% 58/58 [01:03<00:00,  1.10s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:05<00:00,  1.87it/s]\n","                 all         166         606        0.37       0.451       0.335       0.187\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       6/9     14.5G   0.04047   0.01477   0.01337   0.06861        11      1024: 100% 58/58 [01:04<00:00,  1.11s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:04<00:00,  2.22it/s]\n","                 all         166         606       0.302       0.445       0.281      0.0927\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       7/9     14.5G   0.03862   0.01495   0.01252   0.06609        11      1024: 100% 58/58 [01:04<00:00,  1.12s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:06<00:00,  1.74it/s]\n","                 all         166         606       0.388       0.448       0.367       0.202\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       8/9     14.5G   0.03658    0.0151   0.01219   0.06386        31      1024: 100% 58/58 [01:02<00:00,  1.08s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:05<00:00,  2.10it/s]\n","                 all         166         606         0.5       0.435       0.413       0.159\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       9/9     14.5G   0.03542   0.01401    0.0116   0.06102        33      1024: 100% 58/58 [01:03<00:00,  1.09s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:06<00:00,  1.72it/s]\n","                 all         166         606       0.468       0.487       0.449       0.288\n","     0_pepper_flower         166           8        0.37        0.25       0.237      0.0996\n","      1_pepper_young         166         430       0.644       0.726       0.731       0.474\n","    2_pepper_matured         166          35        0.35       0.371       0.269       0.226\n","    3_pepper_covered         166         133        0.51       0.603        0.56       0.354\n","10 epochs completed in 0.217 hours.\n","\n","Optimizer stripped from runs/train/exp2/weights/last.pt, 74.9MB\n","Optimizer stripped from runs/train/exp2/weights/best.pt, 74.9MB\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"57fsVFlsLcht"},"id":"57fsVFlsLcht"},{"cell_type":"markdown","source":["## 載入模型(權重)\n","將訓練好的模型權重，重新載入到yolov7的模型，我們要丟照片做結果呈現測試"],"metadata":{"id":"hTdCLZSqjskC"},"id":"hTdCLZSqjskC"},{"cell_type":"code","source":["#接下來使用hubconf套件中的custom,很容易就可以把我們的YOLOv7模型讀進Python中\n","from hubconf import custom\n","\n","model = custom(path_or_model='runs/train/exp/weights/best.pt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n5psIGPvjr86","executionInfo":{"status":"ok","timestamp":1685859602218,"user_tz":-480,"elapsed":5712,"user":{"displayName":"林德全","userId":"14520221228179753968"}},"outputId":"2c1a427c-b851-4e25-bec9-467a8e6b1495"},"id":"n5psIGPvjr86","execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"]},{"output_type":"stream","name":"stdout","text":["Adding autoShape... \n"]}]},{"cell_type":"markdown","source":["## 結果呈現\n","觀察模型是否有正確框出BBox"],"metadata":{"id":"CvO-ZfQTi8Ma"},"id":"CvO-ZfQTi8Ma"},{"cell_type":"code","source":["#指定照片的檔案路徑及檔名\n","\n","#IMG_FILE = 'VD03/valid/images/DSC_8733.jpg'\n","\n","IMG_FILE = 'VD03/valid/images/DSC_8812.jpg'"],"metadata":{"id":"KcHNs-JZRfnj","executionInfo":{"status":"ok","timestamp":1685859608114,"user_tz":-480,"elapsed":354,"user":{"displayName":"林德全","userId":"14520221228179753968"}}},"id":"KcHNs-JZRfnj","execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["讀取影像檔案"],"metadata":{"id":"ov28LcqmLqQc"},"id":"ov28LcqmLqQc"},{"cell_type":"code","source":["import PIL\n","import sys\n","import numpy as np\n","from IPython.display import Image\n","\n","\n","#讀入照片，轉為numpy array格式。\n","image = PIL.Image.open(IMG_FILE)\n","image = np.asarray(image)\n","image.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PUTV9mc5jEMx","executionInfo":{"status":"ok","timestamp":1685859609865,"user_tz":-480,"elapsed":362,"user":{"displayName":"林德全","userId":"14520221228179753968"}},"outputId":"d24cbaf1-94ca-4772-91bb-33f9ad737df9"},"id":"PUTV9mc5jEMx","execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1280, 1920, 3)"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["送進YOLOv7模型，並顯示結果"],"metadata":{"id":"7OVzyelXLvFU"},"id":"7OVzyelXLvFU"},{"cell_type":"code","source":["results = model([image])\n","\n","PIL.Image.fromarray(results.render()[0])"],"metadata":{"id":"7q9-p3X8jP-7","colab":{"base_uri":"https://localhost:8080/","height":960,"output_embedded_package_id":"1JvsezT359S81v96LkCnPCp6yHuRGnKmb"},"executionInfo":{"status":"ok","timestamp":1685859616714,"user_tz":-480,"elapsed":5027,"user":{"displayName":"林德全","userId":"14520221228179753968"}},"outputId":"dd68a5d6-8b54-40fa-f99f-76dfb1b5a30e"},"id":"7q9-p3X8jP-7","execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"kEBvw_-bjrGc"},"id":"kEBvw_-bjrGc","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"csTHHLNuBfui"},"id":"csTHHLNuBfui"},{"cell_type":"markdown","source":["# 課堂練習\n","在瞭解如何使用YOLO v7預訓練模型來微調訓練任務後，大家可以試著做以下練習:\n","- 試著自己使用標記軟體(例如:LabelImg),練習標記一版自己的label.(或許你自訂的任務只需標記2類，例如: 0_沒有果實, 1_有果實)\n","- 試著重新用你標記的資料來訓練模型"],"metadata":{"id":"jnwuSvbtBZEE"},"id":"jnwuSvbtBZEE"},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"8XaJjqfzBXUT"},"id":"8XaJjqfzBXUT"},{"cell_type":"markdown","source":["# 參考資料\n","\n","- YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors (paper: https://arxiv.org/abs/2207.02696 )\n","- Official YOLOv7 github: https://github.com/WongKinYiu/yolov7\n","- 最新的物件偵測王者 YOLOv7 介紹 - TA家銘 ([https://medium.com/ai-academy-taiwan/206c6adf2e69](https://medium.com/ai-academy-taiwan/206c6adf2e69))\n","- 理解 YOLOv7 預測方式並且部署YOLOv7 模型成為一個服務 ([https://blog.infuseai.io/yolov7-model-deployment-in-primehub-deployment-99b377227447](https://blog.infuseai.io/yolov7-model-deployment-in-primehub-deployment-99b377227447))\n","- [Object Detection_YOLO] YOLOv7 論文筆記 ([https://hackmd.io/@YungHuiHsu/BJ7fpQyps?utm_source=preview-mode&utm_medium=rec](https://hackmd.io/@YungHuiHsu/BJ7fpQyps?utm_source=preview-mode&utm_medium=rec))\n"],"metadata":{"id":"93GzjBoo0VY1"},"id":"93GzjBoo0VY1"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}